{"name":"Walmart Kaggle Competition","tagline":"How I Achieved a Top 25% Score in the Walmart Classification Challenge","body":"### The Walmart Data Science Competition \r\n\r\nEveryone wants to better understand their customers. With the availability of amazing quantities of data from new avenues such as social media as well as traditional avenues such as transactions, it is often difficult to separate the signal from the noise. \r\n\r\nHow can we extract meaning from so much information?\r\n\r\nOne way is to use **machine learning**, or predictive analytics. For example, Walmart uses machine learning to classify the different types of trips that people take to their stores. Customer classification can help Walmart improve store layout, better target promotions through apps, or analyze buying trends.\r\n\r\nAs a recruitment competition on <a href=\"kaggle.com\">Kaggle</a>, **Walmart challenged the data science community to recreate their trip classification system** using only limited transactional data. This could help Walmart innovate and improve upon their machine learning processes.\r\n\r\nWalmart provided over 600,000 rows of **training data**, meaning data already labeled with the corresponding trip classification. The challenge would be to train models to predict upon data without the trip classifications, and Walmart graded participants on their accuracy. My models would be able to predict the correct trip classification with 72% accuracy.\r\n\r\nHere's a snapshot of the initial dataset:\r\n\r\n    data = pd.read_csv(\"train.csv\")\r\n    print(data.head())\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12134928/dddccf70-b3f1-11e5-8966-2b81ed9d0435.png>\r\n\r\n* TripType - 1 of 38 different trip classifications created by Walmart, the \"answer\"\r\n* Visit Number - A unique identifier for a particular trip, with ~90,000 trips in all\r\n* Weekday\r\n* UPC - barcode number \r\n* ScanCount - The number of that particular item purchased, with a return signified as a negative\r\n* Department Description - 1 of 72 item categories created by Walmart\r\n* Fineline Number - 1 of 5000 numbers used to further classify items\r\n\r\nWhen competing in Kaggle competitions or doing any data science project for that matter, it is important to use a structured approach.\r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12157673/a5a06a46-b490-11e5-9276-7f90b5625498.png>\r\n\r\nFeature preparation refers to the process of transforming raw data into data that machine learning models can read and learn from. You then must run this data through your model, improve performance by optimizing the model's parameters, and finally apply this learned model to test data without classifications. I repeated this process tenfold in optimizing my model's performance.\r\n\r\n### Data Analysis - Uncovering the Mystery Behind Walmart's Trip Classification Strategy\r\n\r\nBefore beginning this machine learning process, I wanted to gain some basic understanding of Walmart's data, and specifically the Trip Types that we would need to predict upon. It is always easier to create great features when you know have a story you want to tell your model.\r\n\r\nA great tool to understanding many categories of data in a single glance is a heat map.\r\n\r\n    a4_dims = (13, 9)\r\n    fig, ax = plt.subplots(figsize=a4_dims)\r\n    seaborn.heatmap(ax=ax, data=mytestgrouped_categories_norm.T, linecolor='lightgrey', linewidths=.001)\r\n    heatmap = ax.pcolor(mytestgrouped_categories_norm, cmap=plt.cm.Blues, alpha=0.8)\r\n    ax.invert_yaxis()\r\n    ax.xaxis.tick_top()\r\n    ax.set_yticks(np.arange(mytestgrouped_categories_norm.shape[1]) + 1, minor=False)\r\n    ax.set_xticks(np.arange(mytestgrouped_categories_norm.shape[0]) + 1, minor=False)\r\n    plt.xticks(rotation=90)\r\n    plt.rc('xtick', labelsize=10)\r\n    plt.title('TripType',y=1.04)\r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12152480/3357f490-b472-11e5-8738-3fb0179796f0.png>\r\n\r\nYou can see many correlations between Trip Types and one or two very popular items. \r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12158162/a927eb40-b494-11e5-9647-6e27f102f4bf.png>\r\n \r\nI also created bar graphs for each trip type to explore these associations in more detail.\r\n \r\n    data = pd.read_csv(\"train.csv\")\r\n    type_6 = data[data.TripType == 6]\r\n    type_6_items = type_6[[\"TripType\",\"DepartmentDescription\"]]\r\n    type_6_items.DepartmentDescription.value_counts().head().plot(kind=\"bar\", rot=45, `\r\n                                                             `title=\"Type 6 Trips\", color=\"midnightblue\")\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12152516/6d0369fe-b472-11e5-8c62-cccbcbaf3300.png>\r\n\r\nI discovered that the most important aspect of Walmart's classification were the types of items purchased, with many trips signifying a specific cause, from pet food to party items to men's clothes.\r\n\r\nI could even label many of the 38 trips for my own entertainment:\r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12152574/ac0b0918-b472-11e5-90d9-817d581e7995.png>\r\n\r\n### Feature Preparation\r\n\r\nCreating useful data features for a model to learn from is perhaps the most important part of the data science process. Often, domain expertise can help this process. Based on my knowledge of the data, I created the following features, or variables, for each store visit:\r\n\r\n#### Numerical Weekday Variable\r\n\r\n    data['Weekday'] = data['Weekday'].map({\"Monday\": 1, \"Tuesday\": 2, \"Wednesday\": 3, \"Thursday\": 4, \"Friday\": 5,\r\n    \"Saturday\": 6, \"Sunday\": 7})\r\n\r\n#### 74 Department Description Variables\r\n\r\nThen, I wanted to describe the purchased item in a numerical way. The best way would be to create a column for each of the Department Description types with the data being the amount purchased in that category per trip. Creating columns for every category of data in a column like 'DepartmentDescription' is called creating dummy variables for those values:\r\n\r\n    dummies = pd.get_dummies(data.DepartmentDescription)\r\n    data[dummies.columns] = dummies\r\n\r\nBy default, dummy variables will contain boolean values (0 or 1, with 1 meaning the instance is true), but I instead wanted the values to represent the number of items purchased, so I multiplied the result by the Scan Count:\r\n\r\n    data_dummies = data_dummies.apply(lambda x: x*data[\"ScanCount\"])\r\n\r\n#### Return Variable\r\n\r\nI then created a new feature called Return - signifying whether there was a return with a 1 = a return and 0 = no return. \r\n\r\n    data.loc[data.ScanCount < 0, 'Return'] = 1\r\n    data.loc[data.Return != 1, 'Return'] = 0\r\n\r\n#### Grouping Data\r\n\r\nMachine learning models are typically structured to make predictions or classifications on individual rows in a table. This presented a problem - each row in the Walmart data set represented a **single item**, not a **complete store visit**. As often comes up in data science, I needed to summarize many rows of items data into a single row that would encapsulate the meaning of a particular trip. I used Python's groupby function, and grouped some columns by their max value and others by summing the rows:\r\n   \r\n    grouped_data = data.groupby(\"VisitNumber\")\r\n    grouped_data = grouped.agg({'Weekday': np.max, \"TripType\": np.max, 'NumItems': np.sum, 'Return': np.max, \r\n              '1-HR PHOTO': np.sum, 'ACCESSORIES': np.sum, '(All Other Department Descriptions)': np.sum...})\r\n\r\nThe data now looked like this:\r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12156873/cbbb2afa-b48a-11e5-95b3-a344b9337021.png>\r\n\r\n#### Category Counts\r\n\r\nMy next variable described the total number of unique Department purchases made on each trip. I thought this would be another important variable signifying whether this was an multi-purpose or singular purpose trip.\r\n\r\n    def add_category_counts(data):\r\n        alist = []\r\n        for array in np.asarray(data.iloc[:,4:]):\r\n            count = 0\r\n            for item in array:\r\n                if item > 0:\r\n                    count += 1\r\n            alist.append(count)\r\n        cat_counts = pd.DataFrame(alist)\r\n        cat_counts = cat_counts.rename(columns={0:\"CategoryCount\"})\r\n        cat_counts = cat_counts.set_index(data.index)\r\n        data.insert(4, 'CategoryCounts', cat_counts)\r\n        return data\r\n\r\n#### Fineline Number Dummy Variables\r\n\r\nLastly, I created dummy variables for the most frequent Fineline numbers to limit the size of the dataframe. I later used all of the Fineline Numbers by creating a sparse matrix, a tool from the scikit-learn machine learning library that minimizes the memory required for a dataframe with a large amount of 0 values. The data was simply too large to use a pandas dataframe as before.  In all, I had a dataframe with around 90,000 rows x 5000 columns.\r\n\r\n### Modeling - Applying Machine Learning Techniques\r\n\r\nI tried many different machine learning models throughout the process. I went through logistic regression, Naive Bayes, Random Forest, Extra Trees, and others before landing on the XGBoost library, which produced superior results. The XGBoost library creates multiple decision tree ensembles and averages the results, halting at the best fit. To see the code for the other models, please refer to the project's Github page.\r\n\r\n   import xgboost as xgb\r\n   from sklearn.cross_validation import train_test_split\r\n   from sklearn.metrics import log_loss\r\n\r\nBefore modeling, it is important to split your training data into a training set and a test set, the latter of which hides the answers from the model. This way, it is easy to see how the model performs before unleashing it on brand new data without answers.\r\n\r\n    mytrain, mytest = train_test_split(data, test_size = .4)\r\n---\r\n    dtrain = xgb.DMatrix(np.asarray(mytrain[features]), label = np.asarray(mytrain.TripType))\r\n    dtest = xgb.DMatrix(np.asarray(mytest[features]), label = np.asarray(mytest.TripType))\r\n\r\n### Setting Paramaters\r\nThe XGBoost library provides many customizable parameters to optimize the model for specific circumstances. For instance, it is possible to either output a single prediction for each visit or the probability of every trip type for that particular visit. Since Walmart was using a logloss score as its scoring metric, it was necessary to output the probability of each trip type.\r\n\r\n    num_round = 200\r\n    param = {'objective': 'multi:softprob', 'num_class':38, \r\n         'eval_metric': 'mlogloss', \"max_delta_step\": 5}\r\n    watchlist = [(dtrain,'train'), (dtest, 'eval')]\r\n\r\n### Training the Model\r\n\r\nXGBoost helps prevent overfitting with it's early_stopping_round parameter, which halts the model after it has not improved for more than x number of rounds. Training a model can take anywhere from a few seconds to multiple hours depending on the complexity of an ensemble and size of the data. These rounds usually took a couple minutes.\r\n\r\n    bst = xgb.train(param, dtrain, num_round, watchlist, \r\n                early_stopping_rounds=3)\r\n\r\nIn this case, the model trained for 90 iterations before stopping at the 87th iteration. It's log loss was .847, which translated to an accuracy of 72% when taking the most probable Trip Type for each prediction set.\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12159371/e699cc64-b49e-11e5-8603-b50439b03265.png>\r\n\r\n### Predicting Upon Test Data and Formatting Data to CSV for Competition Submission\r\n\r\n    test_predictions = lr.predict_proba(np.asarray(test_data[test_features]))\r\n\r\n    def predictions_to_csv(test_predictions):\r\n        test_predictions = pd.DataFrame(test_predictions)\r\n        test_indexes = test.index\r\n        test_predictions.insert(0, 'VisitNumber', test_indexes)\r\n        return test_predictions.to_csv(\"submissions/fifth_fineline_xgb.csv\", index=False)\r\n\r\n### Analyzing Results \r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12159470/b4e39f46-b49f-11e5-9588-739769db8f4e.png>\r\n\r\nAs you can see below, machine learning models do much better predicting upon frequent occurrences, and struggle with classifications that are more rare. This is a well-known problem that can really only addressed by gathering more data.\r\n\r\n<img src=https://cloud.githubusercontent.com/assets/14279088/12159474/b8b05376-b49f-11e5-816d-13c206616d17.png>\r\n\r\nI hope this has helped you better understand the machine learning process, and if you are interested, helps you compete in a Kaggle data science competition. You can see the current active competitions at kaggle.com!","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}